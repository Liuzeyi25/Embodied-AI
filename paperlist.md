具身智能论文阅读
Created on 2025/3/29

# VLAs
- OpenVLA: An Open-Source Vision-Language-Action Model. [arXiv](https://arxiv.org/pdf/2406.09246). [2025/3/22]

# Reasoning
- Robotic Control via Embodied Chain-of-Thought Reasoning. [arXiv](https://arxiv.org/pdf/2407.08693) [2025/3/25]
- RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control [arXiv](https://arxiv.org/pdf/2307.15818) [2025/3/28]
- Open X-Embodiment: Robotic Learning Datasets and RT-X Models. [paper](https://openreview.net/pdf?id=zraBtFgxT0) [2025/3/31]
- SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities. [arXiv](https://arxiv.org/pdf/2401.12168)
- EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. [paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/4ec43957eda1126ad4887995d05fae3b-Paper-Conference.pdf) [2025/4/3]
- Any-point Trajectory Modeling for Policy Learning. [arXiv](https://arxiv.org/pdf/2401.00025)
- **[CVPR'25]** CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models. [arXiv](https://arxiv.org/pdf/2503.22020) [web](https://cot-vla.github.io/) [2025/4/2]
> 提出视觉思维链，以预测未来视觉观察作为子目标，并生成动作

# Benchmark
- **[NeurIPS'23]** LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning [paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/8c3c666820ea055a77726d66fc7d447f-Paper-Datasets_and_Benchmarks.pdf)
> LIBERO-SPATIAL\LIBERO-OBJECT\LIBERO-GOAL 各包含10个任务，用于测试空间、物体、任务目标的迁移
> LIBERO-SPATIAL -- 同一个碗在不同的空间关系
