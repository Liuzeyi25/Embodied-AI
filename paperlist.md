具身智能论文阅读
Created on 2025/3/29

# VLAs
1. OpenVLA: An Open-Source Vision-Language-Action Model. [arXiv](https://arxiv.org/pdf/2406.09246). [2025/3/22]

# Reasoning
1. Robotic Control via Embodied Chain-of-Thought Reasoning. [arXiv](https://arxiv.org/pdf/2407.08693) [2025/3/25]
2. RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control [arXiv](https://arxiv.org/pdf/2307.15818) [2025/3/28]
3. Open X-Embodiment: Robotic Learning Datasets and RT-X Models. [paper](https://openreview.net/pdf?id=zraBtFgxT0) [2025/3/31]
4. SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities. [arXiv](https://arxiv.org/pdf/2401.12168)
5. EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. [paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/4ec43957eda1126ad4887995d05fae3b-Paper-Conference.pdf) [2025/4/3]
6. Any-point Trajectory Modeling for Policy Learning. [arXiv](https://arxiv.org/pdf/2401.00025)
7. CVPR'25 CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models. [arXiv](https://arxiv.org/pdf/2503.22020) [web](https://cot-vla.github.io/) [2025/4/2]
   - 提出视觉思维链，以预测未来视觉观察作为子目标，并生成动作

